{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec creation.\n",
    "In the following code, the class 'data_creation_word2vec' is able to to create a word2vec embedding matrix as the input layer for the deep learning architectures. Word2vec types can be from \"Twitter', \"Google', or generating randomly. Training and test dataset can be chosen from 2-class or 3-class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sina\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim-2.2.0-py3.5-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n",
      "2017-12-10 10:25:23,052 : INFO : loading projection weights from GoogleNews-vectors-negative300.bin\n",
      "2017-12-10 10:26:34,949 : INFO : loaded (3000000, 300) matrix from GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google word2vec matrix has been created as the input layer\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import warnings\n",
    "import csv\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# cleaning process to remove any punctuation, parentheses, question marks. This leaves only alphanumeric characters.\n",
    "remove_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "class data_creation_word2vec():\n",
    "    @staticmethod\n",
    "    def clean(sentence):\n",
    "        return re.sub(remove_special_chars, \"\", sentence.lower())\n",
    "\n",
    "    def __init__(self, filename_train, filename_test, word2vec_type, classdataset):\n",
    "        self.classdataset = classdataset\n",
    "        self.word2vec_type = word2vec_type  # 'Google' or 'Twitter', 'random'\n",
    "        # Parse train data\n",
    "        filename = '../LSTM-CNN code/' + filename_train\n",
    "        with open(filename, 'r', encoding='utf-8', newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            training_tweets = []\n",
    "            for tweet in reader:\n",
    "                tweet[2] = data_creation_word2vec.clean(tweet[2])\n",
    "                training_tweets.append(tweet)\n",
    "        self.training_tweets = training_tweets\n",
    "        # Parse test data\n",
    "        filename = '../LSTM-CNN code/' + filename_test\n",
    "        with open(filename, 'r', encoding='utf-8', newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            test_tweets = []\n",
    "            for tweet in reader:\n",
    "                tweet[2] = data_creation_word2vec.clean(tweet[2])\n",
    "                test_tweets.append(tweet)\n",
    "        self.test_tweets = test_tweets\n",
    "\n",
    "        if word2vec_type == 'Twitter':\n",
    "            # word2vec model from twitter based on 400 m tweets. Final results is for 3039345 m words with\n",
    "            # 400-dimension vector\n",
    "            self.Word2Vec_model = KeyedVectors.load_word2vec_format('word2vec_twitter_model.bin', binary=True,\n",
    "                                                                    encoding='latin-1')\n",
    "        if word2vec_type == 'Google' or word2vec_type == 'random':\n",
    "            # word2vec model from pre-trained Google model with 1 m words and 300-dimension vector\n",
    "            self.Word2Vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "    def data_creation(self):\n",
    "        if self.word2vec_type == \"Twitter\" or self.word2vec_type == 'Google':\n",
    "            vectorizer = CountVectorizer(min_df=1, stop_words='english', ngram_range=(1, 1), analyzer=u'word')\n",
    "            analyze = vectorizer.build_analyzer()\n",
    "            # Create Train_Y (i.e., labels)\n",
    "            Train_Y = [int(tweet[0]) for tweet in self.training_tweets]\n",
    "\n",
    "            # Create (Lxd) word embedding matrix corresponding to each tweet. L: number of words in tweet,\n",
    "            # d: word vector dimension\n",
    "            d = self.Word2Vec_model.vector_size\n",
    "            if d == 300:  # means we are using Google word2vec\n",
    "                L = 12  # 90 percentile value of number of words in a tweet based on Google\n",
    "            else:\n",
    "                L = 13  # 90 percentile value of number of words in a tweet based on Twitter\n",
    "\n",
    "            Train_X = np.zeros((len(self.training_tweets), L, d), dtype=np.float32)\n",
    "            for i in range(len(self.training_tweets)):\n",
    "                words_seq = analyze(self.training_tweets[i][2])\n",
    "                index = 0\n",
    "                for word in words_seq:\n",
    "                    if index < L:\n",
    "                        try:\n",
    "                            Train_X[i, index, :] = self.Word2Vec_model[word]\n",
    "                            index += 1\n",
    "                        except KeyError:\n",
    "                            pass\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "\n",
    "            Test_Y_ori = [int(tweet[0]) for tweet in self.test_tweets]\n",
    "            Test_X = np.zeros((len(self.test_tweets), L, d), dtype=np.float32)\n",
    "            for i in range(len(self.test_tweets)):\n",
    "                words_seq = analyze(self.test_tweets[i][2])\n",
    "                index = 0\n",
    "                for word in words_seq:\n",
    "                    if index < L:\n",
    "                        try:\n",
    "                            Test_X[i, index, :] = self.Word2Vec_model[word]\n",
    "                            index += 1\n",
    "                        except KeyError:\n",
    "                            pass\n",
    "                    else:\n",
    "                        break\n",
    "            filename = '1_Word2Vec_' + self.word2vec_type + '_' + self.classdataset + '.pickle'\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump([Train_X, Test_X, Train_Y, Test_Y_ori], f)\n",
    "            print(\"{} word2vec matrix has been created as the input layer\".format(self.word2vec_type))\n",
    "\n",
    "        elif self.word2vec_type == 'random':\n",
    "            # Create randomized word embedding matrix for test and train data\n",
    "            L = 12  # the same as Google\n",
    "            d = 300  # the same as Google\n",
    "            Train_X = np.zeros((len(self.training_tweets), L, d), dtype=np.float32)\n",
    "            max_val = np.amax(self.Word2Vec_model.syn0)\n",
    "            min_val = np.amin(self.Word2Vec_model.syn0)\n",
    "            for i in range(len(self.training_tweets)):\n",
    "                Train_X[i, :, :] = min_val + (max_val - min_val) * np.random.rand(L, d)\n",
    "\n",
    "            Test_X = np.zeros((len(self.test_tweets), L, d), dtype=np.float32)\n",
    "            for i in range(len(self.test_tweets)):\n",
    "                Test_X[i, :, :] = min_val + (max_val - min_val) * np.random.rand(L, d)\n",
    "\n",
    "            Test_Y_ori = [int(tweet[0]) for tweet in self.test_tweets]\n",
    "            Train_Y = [int(tweet[0]) for tweet in self.training_tweets]\n",
    "            filename = '1_Word2Vec_' + self.word2vec_type + '_' + self.classdataset + '.pickle'\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump([Train_X, Test_X, Train_Y, Test_Y_ori], f)\n",
    "            \n",
    "            print(\"{} word2vec matrix has been created as the input layer\".format(self.word2vec_type))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    word2vec = data_creation_word2vec(filename_train='1_TrainingSet_2Class.csv', filename_test='1_TestSet_2Class.csv',\n",
    "                                           word2vec_type='Google', classdataset='2class')\n",
    "\n",
    "    word2vec.data_creation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: CNN-based model with Twitter word2vec model for 3-class dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sina\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1064: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-10 11:01:51,431 : WARNING : From c:\\users\\sina\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1064: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sina\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2578: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-10 11:01:51,537 : WARNING : From c:\\users\\sina\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2578: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sina\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1153: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-10 11:01:51,594 : WARNING : From c:\\users\\sina\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1153: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40942 samples, validate on 10236 samples\n",
      "Epoch 1/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9403Epoch 00000: val_acc improved from -inf to 0.96542, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 14s - loss: 0.1770 - acc: 0.9404 - val_loss: 0.1016 - val_acc: 0.9654\n",
      "Epoch 2/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.1133 - acc: 0.9627Epoch 00001: val_acc improved from 0.96542 to 0.96659, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 8s - loss: 0.1133 - acc: 0.9626 - val_loss: 0.0955 - val_acc: 0.9666\n",
      "Epoch 3/20\n",
      "40768/40942 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9684Epoch 00002: val_acc improved from 0.96659 to 0.96717, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 8s - loss: 0.0966 - acc: 0.9683 - val_loss: 0.0967 - val_acc: 0.9672\n",
      "Epoch 4/20\n",
      "40640/40942 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9723Epoch 00003: val_acc improved from 0.96717 to 0.96923, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 8s - loss: 0.0843 - acc: 0.9722 - val_loss: 0.0944 - val_acc: 0.9692\n",
      "Epoch 5/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9737Epoch 00004: val_acc did not improve\n",
      "40942/40942 [==============================] - 8s - loss: 0.0810 - acc: 0.9737 - val_loss: 0.0962 - val_acc: 0.9682\n",
      "Epoch 6/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9763Epoch 00005: val_acc improved from 0.96923 to 0.97079, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 9s - loss: 0.0735 - acc: 0.9763 - val_loss: 0.0918 - val_acc: 0.9708\n",
      "Epoch 7/20\n",
      "40768/40942 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9775Epoch 00006: val_acc did not improve\n",
      "40942/40942 [==============================] - 8s - loss: 0.0677 - acc: 0.9775 - val_loss: 0.0927 - val_acc: 0.9695\n",
      "Epoch 8/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9791Epoch 00007: val_acc did not improve\n",
      "40942/40942 [==============================] - 8s - loss: 0.0641 - acc: 0.9791 - val_loss: 0.0937 - val_acc: 0.9700\n",
      "Epoch 9/20\n",
      "40768/40942 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9802Epoch 00008: val_acc did not improve\n",
      "40942/40942 [==============================] - 8s - loss: 0.0607 - acc: 0.9802 - val_loss: 0.0934 - val_acc: 0.9704\n",
      "Epoch 10/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9812Epoch 00009: val_acc did not improve\n",
      "40942/40942 [==============================] - 8s - loss: 0.0573 - acc: 0.9813 - val_loss: 0.0988 - val_acc: 0.9708\n",
      "Epoch 11/20\n",
      "40704/40942 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9825Epoch 00010: val_acc did not improve\n",
      "40942/40942 [==============================] - 8s - loss: 0.0534 - acc: 0.9825 - val_loss: 0.0999 - val_acc: 0.9694\n",
      "Epoch 12/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9831Epoch 00011: val_acc improved from 0.97079 to 0.97128, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 8s - loss: 0.0512 - acc: 0.9830 - val_loss: 0.0980 - val_acc: 0.9713\n",
      "Epoch 13/20\n",
      "40640/40942 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9839Epoch 00012: val_acc did not improve\n",
      "40942/40942 [==============================] - 8s - loss: 0.0486 - acc: 0.9839 - val_loss: 0.1030 - val_acc: 0.9704\n",
      "Epoch 14/20\n",
      "40704/40942 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9840Epoch 00013: val_acc did not improve\n",
      "40942/40942 [==============================] - 8s - loss: 0.0483 - acc: 0.9841 - val_loss: 0.1065 - val_acc: 0.9700\n",
      "Epoch 15/20\n",
      "40768/40942 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9852Epoch 00014: val_acc did not improve\n",
      "40942/40942 [==============================] - 8s - loss: 0.0449 - acc: 0.9852 - val_loss: 0.1039 - val_acc: 0.9705\n",
      "Epoch 16/20\n",
      "40704/40942 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9857Epoch 00015: val_acc improved from 0.97128 to 0.97157, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 9s - loss: 0.0442 - acc: 0.9856 - val_loss: 0.1096 - val_acc: 0.9716\n",
      "Epoch 17/20\n",
      "40704/40942 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9866Epoch 00016: val_acc did not improve\n",
      "40942/40942 [==============================] - 8s - loss: 0.0425 - acc: 0.9866 - val_loss: 0.1106 - val_acc: 0.9716\n",
      "Epoch 18/20\n",
      "40704/40942 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9864Epoch 00017: val_acc did not improve\n",
      "40942/40942 [==============================] - 8s - loss: 0.0409 - acc: 0.9865 - val_loss: 0.1097 - val_acc: 0.9699\n",
      "Epoch 19/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9867Epoch 00018: val_acc did not improve\n",
      "40942/40942 [==============================] - 9s - loss: 0.0403 - acc: 0.9867 - val_loss: 0.1113 - val_acc: 0.9706\n",
      "Epoch 20/20\n",
      "40768/40942 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9869Epoch 00019: val_acc did not improve\n",
      "40942/40942 [==============================] - 8s - loss: 0.0401 - acc: 0.9869 - val_loss: 0.1182 - val_acc: 0.9702\n",
      "The optimal epoch size: 15, The value of high accuracy 0.9715709261430246\n",
      "\n",
      "\n",
      "Computation Time 186.8017788846534 seconds\n",
      "\n",
      "\n",
      "Test Accuracy %:  97.15709261430247\n",
      "\n",
      "\n",
      "Confusin matrix:  [[5046   50   24]\n",
      " [  56 3395   45]\n",
      " [  43   73 1504]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.981     0.986     0.983      5120\n",
      "          1      0.965     0.971     0.968      3496\n",
      "          2      0.956     0.928     0.942      1620\n",
      "\n",
      "avg / total      0.971     0.972     0.971     10236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPooling2D, Reshape\n",
    "import pickle\n",
    "from keras.optimizers import Adam\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "start_time = time.clock()\n",
    "\n",
    "# Parse data\n",
    "filename = '../LSTM-CNN code/1_Word2Vec_Twitter_3class.pickle'\n",
    "with open(filename, mode='rb') as f:\n",
    "    Train_X, Test_X, Train_Y, Test_Y_ori = pickle.load(f, encoding='latin1')  # Also can use the encoding 'iso-8859-1'\n",
    "\n",
    "# Construct CNN model.\n",
    "NoClass = len(list(set((Train_Y))))\n",
    "\n",
    "Train_Y = keras.utils.to_categorical(Train_Y, num_classes=NoClass)\n",
    "Test_Y = keras.utils.to_categorical(Test_Y_ori, num_classes=NoClass)\n",
    "\n",
    "L = len(Train_X[0, :, 0])\n",
    "d = len(Train_X[0, 0, :])\n",
    "\n",
    "# Model and Compile\n",
    "model = Sequential()\n",
    "model.add(Reshape((L, d, 1), input_shape=(L, d)))\n",
    "\n",
    "model.add(Conv2D(100, (2, d), strides=(1, 1), padding='valid', activation='relu', use_bias=True))\n",
    "\n",
    "# model.add(Conv2D(100, (2, 1), strides=(1, 1), padding='valid', activation='relu', use_bias=True))\n",
    "\n",
    "output = model.output_shape\n",
    "model.add(MaxPooling2D(pool_size=(output[1], output[2])))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(NoClass, activation='softmax'))\n",
    "\n",
    "# model.add(Dense(NoClass, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))\n",
    "# Optimizer and loss function\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ====================================================================================================\n",
    "# Check point and save the best model\n",
    "filepath = \"weights.best2.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "hist = model.fit(Train_X, Train_Y, epochs=20, batch_size=64, shuffle=False,\n",
    "                            validation_data=(Test_X, Test_Y), callbacks=callbacks_list)\n",
    "\n",
    "# Print the maximum acc_val and its corresponding epoch\n",
    "index = np.argmax(hist.history['val_acc'])\n",
    "print('The optimal epoch size: {}, The value of high accuracy {}'.format(hist.epoch[index], np.max(hist.history['val_acc'])))\n",
    "print('\\n')\n",
    "print('Computation Time', time.clock() - start_time, \"seconds\")\n",
    "print('\\n')\n",
    "\n",
    "# Save the history accuracy results.\n",
    "with open('Accuracy-History-CNN-2class.pickle', 'wb') as f:\n",
    "    pickle.dump([hist.epoch, hist.history['acc'], hist.history['val_acc']], f)\n",
    "\n",
    "# =============================================================================================================\n",
    "# load weights, predict based on the best model, compute accuracy, precision, recall, f-score, confusion matrix.\n",
    "model.load_weights(\"weights.best2.hdf5\")\n",
    "# Compile model (required to make predictions)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Computer confusion matrix, precision, recall.\n",
    "pred = model.predict(Test_X, batch_size=64)\n",
    "y_pred = np.argmax(pred, axis=1)\n",
    "Test_Y_ori = [int(item) for item in Test_Y_ori]\n",
    "print('Test Accuracy %: ', len(np.where(y_pred == np.array(Test_Y_ori))[0])/len(Test_Y_ori) * 100)\n",
    "print('\\n')\n",
    "print('Confusin matrix: ', confusion_matrix(Test_Y_ori, y_pred))\n",
    "print(classification_report(Test_Y_ori, y_pred, digits=3))\n",
    "# =====================================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: LSTM-based model with Google word2vec model for 2-class dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40879 samples, validate on 10221 samples\n",
      "Epoch 1/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.1021 - acc: 0.9726Epoch 00000: val_acc improved from -inf to 0.97955, saving model to weights.best2.hdf5\n",
      "40879/40879 [==============================] - 26s - loss: 0.1021 - acc: 0.9726 - val_loss: 0.0685 - val_acc: 0.9796\n",
      "Epoch 2/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9832Epoch 00001: val_acc improved from 0.97955 to 0.98092, saving model to weights.best2.hdf5\n",
      "40879/40879 [==============================] - 24s - loss: 0.0566 - acc: 0.9832 - val_loss: 0.0615 - val_acc: 0.9809\n",
      "Epoch 3/20\n",
      "40768/40879 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9848- ETA: 0s - loss: 0.0493 - acc:Epoch 00002: val_acc improved from 0.98092 to 0.98131, saving model to weights.best2.hdf5\n",
      "40879/40879 [==============================] - 24s - loss: 0.0496 - acc: 0.9848 - val_loss: 0.0580 - val_acc: 0.9813\n",
      "Epoch 4/20\n",
      "40768/40879 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9864Epoch 00003: val_acc improved from 0.98131 to 0.98190, saving model to weights.best2.hdf5\n",
      "40879/40879 [==============================] - 24s - loss: 0.0453 - acc: 0.9864 - val_loss: 0.0563 - val_acc: 0.9819\n",
      "Epoch 5/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9876Epoch 00004: val_acc improved from 0.98190 to 0.98229, saving model to weights.best2.hdf5\n",
      "40879/40879 [==============================] - 25s - loss: 0.0409 - acc: 0.9876 - val_loss: 0.0552 - val_acc: 0.9823\n",
      "Epoch 6/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9891- ETA: 1s - loEpoch 00005: val_acc improved from 0.98229 to 0.98298, saving model to weights.best2.hdf5\n",
      "40879/40879 [==============================] - 26s - loss: 0.0361 - acc: 0.9891 - val_loss: 0.0578 - val_acc: 0.9830\n",
      "Epoch 7/20\n",
      "40768/40879 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9900Epoch 00006: val_acc did not improve\n",
      "40879/40879 [==============================] - 26s - loss: 0.0326 - acc: 0.9900 - val_loss: 0.0588 - val_acc: 0.9825\n",
      "Epoch 8/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9911Epoch 00007: val_acc did not improve\n",
      "40879/40879 [==============================] - 26s - loss: 0.0310 - acc: 0.9911 - val_loss: 0.0597 - val_acc: 0.9819\n",
      "Epoch 9/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9922Epoch 00008: val_acc did not improve\n",
      "40879/40879 [==============================] - 26s - loss: 0.0271 - acc: 0.9922 - val_loss: 0.0592 - val_acc: 0.9811\n",
      "Epoch 10/20\n",
      "40768/40879 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9930Epoch 00009: val_acc did not improve\n",
      "40879/40879 [==============================] - 26s - loss: 0.0259 - acc: 0.9930 - val_loss: 0.0688 - val_acc: 0.9814\n",
      "Epoch 11/20\n",
      "40768/40879 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9930Epoch 00010: val_acc did not improve\n",
      "40879/40879 [==============================] - 26s - loss: 0.0267 - acc: 0.9930 - val_loss: 0.0767 - val_acc: 0.9822\n",
      "Epoch 12/20\n",
      "40768/40879 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9946Epoch 00011: val_acc did not improve\n",
      "40879/40879 [==============================] - 27s - loss: 0.0220 - acc: 0.9946 - val_loss: 0.0702 - val_acc: 0.9818\n",
      "Epoch 13/20\n",
      "40768/40879 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9949Epoch 00012: val_acc did not improve\n",
      "40879/40879 [==============================] - 27s - loss: 0.0217 - acc: 0.9949 - val_loss: 0.0796 - val_acc: 0.9824\n",
      "Epoch 14/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9956Epoch 00013: val_acc improved from 0.98298 to 0.98327, saving model to weights.best2.hdf5\n",
      "40879/40879 [==============================] - 28s - loss: 0.0206 - acc: 0.9956 - val_loss: 0.0794 - val_acc: 0.9833\n",
      "Epoch 15/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9969- ETA: 0s - loss: 0.0172 - acc: Epoch 00014: val_acc did not improve\n",
      "40879/40879 [==============================] - 72s - loss: 0.0173 - acc: 0.9969 - val_loss: 0.0835 - val_acc: 0.9824\n",
      "Epoch 16/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9966Epoch 00015: val_acc did not improve\n",
      "40879/40879 [==============================] - 80s - loss: 0.0178 - acc: 0.9966 - val_loss: 0.0867 - val_acc: 0.9821\n",
      "Epoch 17/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9974- ETA: 3s Epoch 00016: val_acc did not improve\n",
      "40879/40879 [==============================] - 78s - loss: 0.0157 - acc: 0.9974 - val_loss: 0.0866 - val_acc: 0.9819\n",
      "Epoch 18/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9973Epoch 00017: val_acc did not improve\n",
      "40879/40879 [==============================] - 71s - loss: 0.0166 - acc: 0.9973 - val_loss: 0.0827 - val_acc: 0.9824\n",
      "Epoch 19/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9974Epoch 00018: val_acc did not improve\n",
      "40879/40879 [==============================] - 56s - loss: 0.0160 - acc: 0.9974 - val_loss: 0.0896 - val_acc: 0.9832\n",
      "Epoch 20/20\n",
      "40832/40879 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9976Epoch 00019: val_acc did not improve\n",
      "40879/40879 [==============================] - 54s - loss: 0.0161 - acc: 0.9976 - val_loss: 0.0779 - val_acc: 0.9820\n",
      "\n",
      "\n",
      "The optimal epoch size: 13, The value of high accuracy 0.9832697387731142\n",
      "\n",
      "\n",
      "Computation Time 787.7704465265961 seconds\n",
      "\n",
      "\n",
      "Test Accuracy %:  98.32697387731142\n",
      "\n",
      "\n",
      "Confusin matrix:  [[5016   94]\n",
      " [  77 5034]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.985     0.982     0.983      5110\n",
      "          1      0.982     0.985     0.983      5111\n",
      "\n",
      "avg / total      0.983     0.983     0.983     10221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPooling2D\n",
    "import pickle\n",
    "from keras.optimizers import SGD, Adam\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "# parse data\n",
    "filename = '../LSTM-CNN code/1_Word2Vec_Google_2class.pickle'\n",
    "with open(filename, mode='rb') as f:\n",
    "    Train_X, Test_X, Train_Y, Test_Y_ori = pickle.load(f, encoding='latin1')  # Also can use the encoding 'iso-8859-1'\n",
    "\n",
    "# Construct and compile the LSTM model\n",
    "NoClass = len(list(set(Train_Y)))\n",
    "\n",
    "Train_Y = keras.utils.to_categorical(Train_Y, num_classes=NoClass)\n",
    "Test_Y = keras.utils.to_categorical(Test_Y_ori, num_classes=NoClass)\n",
    "\n",
    "L = len(Train_X[0, :, 0])\n",
    "d = len(Train_X[0, 0, :])\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(GRU(32, return_sequences=False, input_shape=(L, d)))\n",
    "model.add(LSTM(50, return_sequences=False, input_shape=(L, d)))  # returns a sequence of vectors with dimension of units\n",
    "#model.add(LSTM(32))\n",
    "model.add(Dropout(.50))\n",
    "\n",
    "model.add(Dense(NoClass, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ====================================================================================================\n",
    "# Training, check point and save the best model\n",
    "filepath = \"weights.best2.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "hist = model.fit(Train_X, Train_Y, epochs=20, batch_size=64, shuffle=False,\n",
    "                            validation_data=(Test_X, Test_Y), callbacks=callbacks_list)\n",
    "\n",
    "# Print the maximum acc_val and its corresponding epoch\n",
    "index = np.argmax(hist.history['val_acc'])\n",
    "print('\\n')\n",
    "print('The optimal epoch size: {}, The value of high accuracy {}'.format(hist.epoch[index], np.max(hist.history['val_acc'])))\n",
    "print('\\n')\n",
    "print('Computation Time', time.clock() - start_time, \"seconds\")\n",
    "print('\\n')\n",
    "\n",
    "# =============================================================================================================\n",
    "# load weights, predict based on the best model, compute accuracy, precision, recall, f-score, confusion matrix.\n",
    "model.load_weights(\"weights.best2.hdf5\")\n",
    "# Compile model (required to make predictions)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Computer confusion matrix, precision, recall.\n",
    "pred = model.predict(Test_X, batch_size=64)\n",
    "y_pred = np.argmax(pred, axis=1)\n",
    "Test_Y_ori = [int(item) for item in Test_Y_ori]\n",
    "print('Test Accuracy %: ', len(np.where(y_pred == np.array(Test_Y_ori))[0])/len(Test_Y_ori) * 100)\n",
    "print('\\n')\n",
    "print('Confusin matrix: ', confusion_matrix(Test_Y_ori, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(Test_Y_ori, y_pred, digits=3))\n",
    "# =====================================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: CNN+LSTM model with Google word2vec model for 2-class dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40942 samples, validate on 10236 samples\n",
      "Epoch 1/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.9166Epoch 00000: val_acc improved from -inf to 0.95955, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 67s - loss: 0.2455 - acc: 0.9167 - val_loss: 0.1353 - val_acc: 0.9596\n",
      "Epoch 2/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.1145 - acc: 0.9671Epoch 00001: val_acc improved from 0.95955 to 0.96581, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 24s - loss: 0.1144 - acc: 0.9671 - val_loss: 0.1110 - val_acc: 0.9658\n",
      "Epoch 3/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0973 - acc: 0.9712Epoch 00002: val_acc improved from 0.96581 to 0.96913, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 24s - loss: 0.0972 - acc: 0.9712 - val_loss: 0.1115 - val_acc: 0.9691\n",
      "Epoch 4/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9748Epoch 00003: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.0855 - acc: 0.9748 - val_loss: 0.1037 - val_acc: 0.9689\n",
      "Epoch 5/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9777Epoch 00004: val_acc improved from 0.96913 to 0.97118, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 23s - loss: 0.0775 - acc: 0.9777 - val_loss: 0.1034 - val_acc: 0.9712\n",
      "Epoch 6/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9805Epoch 00005: val_acc did not improve\n",
      "40942/40942 [==============================] - 24s - loss: 0.0706 - acc: 0.9805 - val_loss: 0.1151 - val_acc: 0.9669\n",
      "Epoch 7/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9822Epoch 00006: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.0651 - acc: 0.9822 - val_loss: 0.1135 - val_acc: 0.9678\n",
      "Epoch 8/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9839- ETA: 0s - loss: 0.0603 - Epoch 00007: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.0599 - acc: 0.9839 - val_loss: 0.1162 - val_acc: 0.9682\n",
      "Epoch 9/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9851Epoch 00008: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.0569 - acc: 0.9851 - val_loss: 0.1191 - val_acc: 0.9696\n",
      "Epoch 10/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9868Epoch 00009: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.0519 - acc: 0.9868 - val_loss: 0.1367 - val_acc: 0.9640\n",
      "Epoch 11/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9877Epoch 00010: val_acc did not improve\n",
      "40942/40942 [==============================] - 24s - loss: 0.0503 - acc: 0.9877 - val_loss: 0.1179 - val_acc: 0.9690\n",
      "Epoch 12/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9883Epoch 00011: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.0484 - acc: 0.9884 - val_loss: 0.1259 - val_acc: 0.9696\n",
      "Epoch 13/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9897Epoch 00012: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.0440 - acc: 0.9897 - val_loss: 0.1224 - val_acc: 0.9672\n",
      "Epoch 14/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9900Epoch 00013: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.0427 - acc: 0.9900 - val_loss: 0.1303 - val_acc: 0.9679\n",
      "Epoch 15/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9898Epoch 00014: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.0438 - acc: 0.9898 - val_loss: 0.1209 - val_acc: 0.9705\n",
      "Epoch 16/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9915- ETA: 0s - loss: 0.0394 - aEpoch 00015: val_acc did not improve\n",
      "40942/40942 [==============================] - 24s - loss: 0.0392 - acc: 0.9915 - val_loss: 0.1383 - val_acc: 0.9702\n",
      "Epoch 17/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9913Epoch 00016: val_acc improved from 0.97118 to 0.97196, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 22s - loss: 0.0394 - acc: 0.9914 - val_loss: 0.1198 - val_acc: 0.9720\n",
      "Epoch 18/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9918- EEpoch 00017: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.0384 - acc: 0.9918 - val_loss: 0.1221 - val_acc: 0.9705\n",
      "Epoch 19/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9922Epoch 00018: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.0367 - acc: 0.9922 - val_loss: 0.1321 - val_acc: 0.9697\n",
      "Epoch 20/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9928Epoch 00019: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.0348 - acc: 0.9927 - val_loss: 0.1316 - val_acc: 0.9708\n",
      "\n",
      "\n",
      "The optimal epoch size: 16, The value of high accuracy 0.9719617037905431\n",
      "\n",
      "\n",
      "Computation Time 529.6284419272856 seconds\n",
      "\n",
      "\n",
      "Test Accuracy %:  97.19617037905431\n",
      "\n",
      "\n",
      "Confusin matrix:  [[5028   60   32]\n",
      " [  42 3418   36]\n",
      " [  42   75 1503]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.984     0.982     0.983      5120\n",
      "          1      0.962     0.978     0.970      3496\n",
      "          2      0.957     0.928     0.942      1620\n",
      "\n",
      "avg / total      0.972     0.972     0.972     10236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, Conv1D, Flatten, MaxPooling2D, Lambda, Reshape\n",
    "import pickle\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from keras.layers import LSTM\n",
    "from keras import regularizers\n",
    "from keras.backend import mean\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "# Parse data\n",
    "filename = '../LSTM-CNN code/1_Word2Vec_Twitter_3class.pickle'\n",
    "with open(filename, mode='rb') as f:\n",
    "    Train_X, Test_X, Train_Y, Test_Y_ori = pickle.load(f, encoding='latin1')  # Also can use the encoding 'iso-8859-1'\n",
    "\n",
    "# Construct and compile the LSTM model\n",
    "NoClass = len(list(set(Train_Y)))\n",
    "\n",
    "Train_Y = keras.utils.to_categorical(Train_Y, num_classes=NoClass)\n",
    "Test_Y = keras.utils.to_categorical(Test_Y_ori, num_classes=NoClass)\n",
    "\n",
    "L = len(Train_X[0, :, 0])\n",
    "d = len(Train_X[0, 0, :])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape((L, d, 1), input_shape=(L, d)))\n",
    "model.add(Conv2D(100, (2, d), strides=(1, 1), padding='valid', activation='relu', use_bias=True))\n",
    "output = model.output_shape\n",
    "model.add(Reshape((output[1], output[3])))\n",
    "model.add(Dropout(.25))\n",
    "model.add(LSTM(100, return_sequences=False, activation='tanh', recurrent_activation='hard_sigmoid'))\n",
    "model.add(Dropout(.50))\n",
    "model.add(Dense(NoClass, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ====================================================================================================\n",
    "# Training, check point and save the best model\n",
    "filepath = \"weights.best2.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "hist = model.fit(Train_X, Train_Y, epochs=20, batch_size=64, shuffle=False,\n",
    "                            validation_data=(Test_X, Test_Y), callbacks=callbacks_list)\n",
    "\n",
    "# Print the maximum acc_val and its corresponding epoch\n",
    "index = np.argmax(hist.history['val_acc'])\n",
    "print('\\n')\n",
    "print('The optimal epoch size: {}, The value of high accuracy {}'.format(hist.epoch[index], np.max(hist.history['val_acc'])))\n",
    "print('\\n')\n",
    "print('Computation Time', time.clock() - start_time, \"seconds\")\n",
    "print('\\n')\n",
    "\n",
    "# =============================================================================================================\n",
    "# load weights, predict based on the best model, compute accuracy, precision, recall, f-score, confusion matrix.\n",
    "model.load_weights(\"weights.best2.hdf5\")\n",
    "# Compile model (required to make predictions)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Computer confusion matrix, precision, recall.\n",
    "pred = model.predict(Test_X, batch_size=64)\n",
    "y_pred = np.argmax(pred, axis=1)\n",
    "Test_Y_ori = [int(item) for item in Test_Y_ori]\n",
    "print('Test Accuracy %: ', len(np.where(y_pred == np.array(Test_Y_ori))[0])/len(Test_Y_ori) * 100)\n",
    "print('\\n')\n",
    "print('Confusin matrix: ', confusion_matrix(Test_Y_ori, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(Test_Y_ori, y_pred, digits=3))\n",
    "# =====================================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: CNN+LSTM model with random word2vec model for 3-class dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40942 samples, validate on 10236 samples\n",
      "Epoch 1/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 1.0315 - acc: 0.4980Epoch 00000: val_acc improved from -inf to 0.50020, saving model to weights.best2.hdf5\n",
      "40942/40942 [==============================] - 26s - loss: 1.0315 - acc: 0.4980 - val_loss: 1.0097 - val_acc: 0.5002\n",
      "Epoch 2/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 1.0052 - acc: 0.5003- ETA: 0s - loss: 1.0053 - accEpoch 00001: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 1.0052 - acc: 0.5003 - val_loss: 1.0100 - val_acc: 0.5002\n",
      "Epoch 3/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.9878 - acc: 0.5083Epoch 00002: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.9878 - acc: 0.5083 - val_loss: 1.0212 - val_acc: 0.4827\n",
      "Epoch 4/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.9537 - acc: 0.5306Epoch 00003: val_acc did not improve\n",
      "40942/40942 [==============================] - 24s - loss: 0.9537 - acc: 0.5306 - val_loss: 1.0453 - val_acc: 0.4679\n",
      "Epoch 5/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.9050 - acc: 0.5684Epoch 00004: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.9050 - acc: 0.5683 - val_loss: 1.0699 - val_acc: 0.4501\n",
      "Epoch 6/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.8474 - acc: 0.6119Epoch 00005: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.8475 - acc: 0.6118 - val_loss: 1.1076 - val_acc: 0.4407\n",
      "Epoch 7/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.7940 - acc: 0.6464Epoch 00006: val_acc did not improve\n",
      "40942/40942 [==============================] - 22s - loss: 0.7940 - acc: 0.6464 - val_loss: 1.1489 - val_acc: 0.4150\n",
      "Epoch 8/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.7434 - acc: 0.6778Epoch 00007: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.7434 - acc: 0.6777 - val_loss: 1.1966 - val_acc: 0.4132\n",
      "Epoch 9/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.7036 - acc: 0.6998- ETA: 1s - loss: 0.698Epoch 00008: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.7038 - acc: 0.6998 - val_loss: 1.2103 - val_acc: 0.4115\n",
      "Epoch 10/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.6726 - acc: 0.7143- ETAEpoch 00009: val_acc did not improve\n",
      "40942/40942 [==============================] - 23s - loss: 0.6727 - acc: 0.7143 - val_loss: 1.2536 - val_acc: 0.4143\n",
      "Epoch 11/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.6437 - acc: 0.7339Epoch 00010: val_acc did not improve\n",
      "40942/40942 [==============================] - 22s - loss: 0.6436 - acc: 0.7340 - val_loss: 1.2767 - val_acc: 0.4209\n",
      "Epoch 12/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.6251 - acc: 0.7450Epoch 00011: val_acc did not improve\n",
      "40942/40942 [==============================] - 22s - loss: 0.6251 - acc: 0.7449 - val_loss: 1.3212 - val_acc: 0.4185\n",
      "Epoch 13/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.6069 - acc: 0.7525Epoch 00012: val_acc did not improve\n",
      "40942/40942 [==============================] - 22s - loss: 0.6069 - acc: 0.7525 - val_loss: 1.3675 - val_acc: 0.4296\n",
      "Epoch 14/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.5893 - acc: 0.7619Epoch 00013: val_acc did not improve\n",
      "40942/40942 [==============================] - 22s - loss: 0.5892 - acc: 0.7621 - val_loss: 1.3784 - val_acc: 0.4190\n",
      "Epoch 15/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.5773 - acc: 0.7695Epoch 00014: val_acc did not improve\n",
      "40942/40942 [==============================] - 22s - loss: 0.5773 - acc: 0.7696 - val_loss: 1.3815 - val_acc: 0.4221\n",
      "Epoch 16/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.5609 - acc: 0.7789- ETA: 1s - loss:Epoch 00015: val_acc did not improve\n",
      "40942/40942 [==============================] - 25s - loss: 0.5609 - acc: 0.7789 - val_loss: 1.4203 - val_acc: 0.4264\n",
      "Epoch 17/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.5500 - acc: 0.7823Epoch 00016: val_acc did not improve\n",
      "40942/40942 [==============================] - 22s - loss: 0.5499 - acc: 0.7824 - val_loss: 1.4164 - val_acc: 0.4160\n",
      "Epoch 18/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.7900Epoch 00017: val_acc did not improve\n",
      "40942/40942 [==============================] - 22s - loss: 0.5388 - acc: 0.7900 - val_loss: 1.4778 - val_acc: 0.4149\n",
      "Epoch 19/20\n",
      "40896/40942 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.7973Epoch 00018: val_acc did not improve\n",
      "40942/40942 [==============================] - 22s - loss: 0.5223 - acc: 0.7973 - val_loss: 1.5051 - val_acc: 0.4185\n",
      "Epoch 20/20\n",
      "40832/40942 [============================>.] - ETA: 0s - loss: 0.5170 - acc: 0.7987Epoch 00019: val_acc did not improve\n",
      "40942/40942 [==============================] - 22s - loss: 0.5168 - acc: 0.7988 - val_loss: 1.4769 - val_acc: 0.4174\n",
      "\n",
      "\n",
      "The optimal epoch size: 0, The value of high accuracy 0.5001953889868044\n",
      "\n",
      "\n",
      "Computation Time 474.5191373902762 seconds\n",
      "\n",
      "\n",
      "Test Accuracy %:  50.019538882375926\n",
      "\n",
      "\n",
      "Confusin matrix:  [[5120    0    0]\n",
      " [3496    0    0]\n",
      " [1620    0    0]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.500     1.000     0.667      5120\n",
      "          1      0.000     0.000     0.000      3496\n",
      "          2      0.000     0.000     0.000      1620\n",
      "\n",
      "avg / total      0.250     0.500     0.334     10236\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sina\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, Conv1D, Flatten, MaxPooling2D, Lambda, Reshape\n",
    "import pickle\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from keras.layers import LSTM\n",
    "from keras import regularizers\n",
    "from keras.backend import mean\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "# Parse data\n",
    "filename = '../LSTM-CNN code/1_Word2Vec_random_3class.pickle'\n",
    "with open(filename, mode='rb') as f:\n",
    "    Train_X, Test_X, Train_Y, Test_Y_ori = pickle.load(f, encoding='latin1')  # Also can use the encoding 'iso-8859-1'\n",
    "\n",
    "# Construct and compile the LSTM model\n",
    "NoClass = len(list(set(Train_Y)))\n",
    "\n",
    "Train_Y = keras.utils.to_categorical(Train_Y, num_classes=NoClass)\n",
    "Test_Y = keras.utils.to_categorical(Test_Y_ori, num_classes=NoClass)\n",
    "\n",
    "L = len(Train_X[0, :, 0])\n",
    "d = len(Train_X[0, 0, :])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape((L, d, 1), input_shape=(L, d)))\n",
    "model.add(Conv2D(100, (2, d), strides=(1, 1), padding='valid', activation='relu', use_bias=True))\n",
    "output = model.output_shape\n",
    "model.add(Reshape((output[1], output[3])))\n",
    "model.add(Dropout(.25))\n",
    "model.add(LSTM(100, return_sequences=False, activation='tanh', recurrent_activation='hard_sigmoid'))\n",
    "model.add(Dropout(.50))\n",
    "model.add(Dense(NoClass, activation='softmax', kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ====================================================================================================\n",
    "# Training, check point and save the best model\n",
    "filepath = \"weights.best2.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "hist = model.fit(Train_X, Train_Y, epochs=20, batch_size=64, shuffle=False,\n",
    "                            validation_data=(Test_X, Test_Y), callbacks=callbacks_list)\n",
    "\n",
    "# Print the maximum acc_val and its corresponding epoch\n",
    "index = np.argmax(hist.history['val_acc'])\n",
    "print('\\n')\n",
    "print('The optimal epoch size: {}, The value of high accuracy {}'.format(hist.epoch[index], np.max(hist.history['val_acc'])))\n",
    "print('\\n')\n",
    "print('Computation Time', time.clock() - start_time, \"seconds\")\n",
    "print('\\n')\n",
    "\n",
    "# =============================================================================================================\n",
    "# load weights, predict based on the best model, compute accuracy, precision, recall, f-score, confusion matrix.\n",
    "model.load_weights(\"weights.best2.hdf5\")\n",
    "# Compile model (required to make predictions)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Computer confusion matrix, precision, recall.\n",
    "pred = model.predict(Test_X, batch_size=64)\n",
    "y_pred = np.argmax(pred, axis=1)\n",
    "Test_Y_ori = [int(item) for item in Test_Y_ori]\n",
    "print('Test Accuracy %: ', len(np.where(y_pred == np.array(Test_Y_ori))[0])/len(Test_Y_ori) * 100)\n",
    "print('\\n')\n",
    "print('Confusin matrix: ', confusion_matrix(Test_Y_ori, y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(Test_Y_ori, y_pred, digits=3))\n",
    "# =====================================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of number of words per tweet for both Google and Twitter word2vec models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-10 11:58:11,626 : INFO : loading projection weights from word2vec_twitter_model.bin\n",
      "2017-12-10 11:59:14,750 : INFO : loaded (3039345, 400) matrix from word2vec_twitter_model.bin\n",
      "2017-12-10 11:59:20,914 : INFO : loading projection weights from GoogleNews-vectors-negative300.bin\n",
      "2017-12-10 12:00:09,791 : INFO : loaded (3000000, 300) matrix from GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of number of words for Google is 8.480246581374299 and Twitter is 8.82117957875682: \n",
      "Median of number of words for Google is 8.0 and Twitter is 9.0: \n",
      "90 quantile of number of words for Google is 12.0 and Twitter is 13.0: \n",
      "Max of number of words for Google is 20 and Twitter is 21: \n",
      "Min of number of words for Google is 0 and Twitter is 0: \n",
      "Average number of missing words:  1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEOCAYAAACjJpHCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aadf3d87f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEOCAYAAABxdpuaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAH79JREFUeJzt3Xu8VVW99/HPFxAVFHwUUIQQs07CyVNHN2kSeSkpXqbn\nSHazUs9RKdFO6JG0EiNOWWoXyh4oelK74NFOWZl5yS5KmZFIkpdMOilmiuD9hrqB3/PHGNsWm7X3\nnmOz19p76/f9evFizrHnmGOsteaavzXmmHMMRQRmZmYlBvR2BczMrP9x8DAzs2IOHmZmVszBw8zM\nijl4mJlZMQcPMzMr5uBhZmbFHDzMzKyYg4eZmRUb1KyCJC0D9gGeBcYCE4FpwFbA4oi4JW93ZP7b\nDsD8iLg3p58IDAdGAfMi4rHOyhsxYkSMHz++MS/GzOxF6uabb34oIkZ2tZ2aMTyJpAOAocByoJUU\nQH4OTAYGAldHxMGSxgIXRMRUSTsDCyNiuqRJwMkRcYykFuB9ETGrszJbWlpi2bJljXxZZmYvOpJu\njoiWrrZr1mWrDwFvAIZGxMPAYcDKiNgQEc8DrZImA+8DlgJExIPAhBxQjgOW5H0tB46SNLhJdTcz\ns3YaHjwkDQT+AEwCfi/pINLlqzU1m60GJtRJXwPsWZseERuBdcC4RtfdzMzqa3jwyK2LeRFxCHA6\n8HVS38WjNZu1kvoyStPNzKwXNPVuq4hYCDwBCNim5k9DgMeBRwrTNyFphqRlkpatXbu2h2tvZmZt\neuNW3RXAdcCuNWljSX0ZK9qlj2mfnvs6tgXuar/jiFgUES0R0TJyZJc3C5iZWTc1o89jmKQJeXkk\ncCvwQ2AvSQMkbU26ZXgpcDHpDiwkjQZuj4gHgIuAg/Iu9wYujYjWRtfdzMzqa8ZzHq8ErpD0M+AG\n4EsRsUHSWcDZwHrghNwRvkrSJZLmkG7tPR4gIm6UNEXSbGAnYHYT6m1mZh1oynMevcHPeZiZletr\nz3mYmdmLSNOGJzHrLTNnzmx6mQsWLGh6mWbN5JaHmZkVc/AwM7NiDh5mZlbMwcPMzIo5eJiZWTEH\nDzMzK+bgYWZmxRw8zMysmIOHmZkVc/AwM7NiDh5mZlbMwcPMzIo5eJiZWTEHDzMzK+bgYWZmxRw8\nzMysmIOHmZkVc/AwM7NinobW+o3emE7WzOpzy8PMzIq55WHWAFvSSlqwYEEP1sSsMdzyMDOzYg4e\nZmZWzMHDzMyKOXiYmVkxBw8zMyvm4GFmZsUcPMzMrJiDh5mZFXPwMDOzYg4eZmZWrGnDk0j6KLB1\nRMyVNAWYBmwFLI6IW/I2RwITgR2A+RFxb04/ERgOjALmRcRjzaq3WbN1d2gTD2tizdSU4CHpZcDx\nwLclDQXOAyYDA4GrgYMljQVmRMRUSTsDC4HpkiYB+0XEMZJagLnArGbU28zM6mvWZasPAJfk5cOA\nlRGxISKeB1olTQbeBywFiIgHgQk5oBwHLMl5lwNHSRrcpHqbmVkdDQ8ekqYCvwBac9I+wJqaTVYD\nE+qkrwH2rE2PiI3AOmBcY2ttZmadaWjwyC2E/SPiFzXJw4FHa9ZbSX0Zpen1ypshaZmkZWvXru2B\nV2BmZvU0uuUxA/hau7RHgG1q1ocAj3cjfTMRsSgiWiKiZeTIkVtYdTMz60ijO8yPAU6TBOkOKoAv\nArvVbDOW1JcxDNi1Jn1MTl/Rlp5bMtsCdzW01tYwnkrW7MWhocEjIia1LUuamxc/B1wnaQDpVt1B\npI7y+4FFedvRwO0R8YCki0gtmG8BewOXRkRb/4mZmfWCpk9DGxFPSzoLOBtYD5yQO8JXSbpE0hxg\nKOnWXiLiRklTJM0GdgJmN7vOZma2qaYFj4iYW7N8FXBVnW0u7CDvuY2rmZmZlfLwJGZmVszBw8zM\nijl4mJlZMQcPMzMr5uBhZmbFHDzMzKyYg4eZmRVz8DAzs2IOHmZmVszBw8zMijl4mJlZMQcPMzMr\n5uBhZmbFHDzMzKyYg4eZmRVz8DAzs2IOHmZmVszBw8zMijl4mJlZMQcPMzMr5uBhZmbFHDzMzKyY\ng4eZmRVz8DAzs2IOHmZmVmxQb1fAzHrGzJkzu513wYIFPVgTeylwy8PMzIo5eJiZWTEHDzMzK+bg\nYWZmxRw8zMysmO+2sm7Zkjt7zKz/c8vDzMyKVWp5SNonIm7ubiGSzgCOB54ApkfEPZKmANOArYDF\nEXFL3vZIYCKwAzA/Iu7N6ScCw4FRwLyIeKy79TEzsy1TteVxjqR5kg6RpJICJE0ErgJeCdwJfFjS\nUOA8YA7wceALeduxwIyImAecA8zP6ZOA/SLis8DFwNySOpiZWc+q2udxeEQ8k1sLcyRtAK6IiBVd\nZYyIO9qWJS0FNgCHASsjYgOwQVKrpMnAFGBpzvegpAk5oBwHLMm7WQ5cKekjEfF8xfqbmVkPKu3z\neALYDTgZOE7SGZJOkNRlEJK0NbAH8FVgH2BNzZ9XAxPqpK8B9qxNj4iNwDpgXGHdzcysh1RteVwm\naRgwBFgA/EdEPA0gaV/gUuDtHWWWNAQ4G/ggKQgMB+6t2aSV1JcxHHi0IP3P7cqZAcwAGDfOscXM\nrFGqtjxGAqdHxGsjYlFb4Mg2kloNHYqIZyJiFnAocCrwCLBNzSZDgMe7kd6+nEUR0RIRLSNHjqz4\n0szMrFTVlsc04CkASTsC6yJiHUBE3ES6O6pLEXGtpIeAFcAhNX8aS+rLGAbsWpM+JqevaEuXNBjY\nFrirYt3NzKyHVW15fBS4Ji8/SurvmFolo6RtJG2bl8cCVwCXA3tJGpD7QgaROsovBibnbUcDt0fE\nA8BFwEF5l3sDl0ZEa8W6m5lZD6va8hgEHAwQESHpG6TbbnerkHc6cLak7wIPAx+LiGclnUXqB1kP\nnJA7wldJukTSHGAo6dkQIuJGSVMkzQZ2AmZXf4lmZtbTqgaPNaS+DfJzHqeSTvpdioiLSS2K9ulX\nkZ7/aJ9+YQf7ObdiXc3MrMGqBo8fAEvy84F7AAEc1ahKmZlZ31YpeETEbZIOAF4FDCRdstq+kRUz\nM7O+q+rYVuNIfRfbA23Dk7wOeFuD6mVmZn1Y1butfkp6lmMAKXiIdOnKzMxegqr2edwcER+oTZC0\newPqY2Zm/UDV4HGrpHOB22rSpgAn9HyVzMysr6saPPYjDUZYO+bHa3q+OmZm1h9UDR6z8gROA/LD\nfEga08B6mZlZH1a1w3xXSbcB35U0SNIpwOgG1svMzPqwqi2PeaQhQfaIiPWSLgV+RsUBEc3M7MWl\nasvjqjycyBN5/TXAiMZUyczM+rqqLY+HJH0RGC7pVcBMPI+4mdlLVtXhSb4p6SbSsOiDgIMj4vcN\nrZmZmfVZlS5b5eFJngJ+TBok8RlJJzeyYmZm1ndVvWx1B2lY9rZxrbYD7ga+0ohKmZlZ31Y1eBwY\nEcvaViSNB97ciAqZmVnfV+myVW3gyJ4jTU1rZmYvQVWHZN9IGkW37bLVE8D5jaqUmZn1bVUvW709\nIn7Q0R8lbRURrT1UJzMz6+NKnvN4Yyd/PwSY0wP1MTOzfqBq8PgEsAPwZF7fBVhd8/dX4OBhZvaS\nUTV4LI+Ij7StSBoGnB4RH8/rHuPKzOwlpOrYVo+1Wx8GHNe2EhF39FiNzMysz6va8vi1pB8AdwLb\nA+8ALmlYrcysqWbOnNmtfAsWLOjhmlh/UXVsqyWS7gTeBuwIvDsiftnQmpmZWZ9VdWyrUaQO8ZcD\nXwZ2l7RjIytmZmZ9V9U+j28DzwCtEfE88Cvg0obVyszM+rSqweP6iDgduCevbwNMakiNzMysz6sa\nPJA0HdhZ0iHAd4DLGlYrMzPr06oGj3OAUaTWxkzgQuCDjaqUmZn1bVVv1f0+MDsivtrIypiZWf9Q\nNXg8CjxYmyCppc5Q7daPdPfefjOzqsFjJ+AWSffk9YHABNKlrC5JmgecDNwLHB0Rf5A0BZgGbAUs\njohb8rZHAhNJY2nNj4h7c/qJwPBc5ryIaP/Uu5mZNUmHwUPSP+XFO4DlwELS7bptDq1SgKT9gVuB\n0cBngQskHQCcB0wmBaKrgYMljQVmRMRUSTvnMqdLmgTsFxHHSGoB5gKzKr9KMzPrUZ21PL4P/GtE\nrJd0FbCO9JzHHwEk3VCxjPsj4jc5zzzgfuAwYGVEbAA2SGqVNBmYAiwFiIgHJU3IAeU4YEne33Lg\nSkkfyc+cmJlZk3V2t9XFEXF7Xn4EuIC/D8lORKyvUkBE3FOzuh1wE7APsKYmfTXpMlj79DXAnrXp\nEbGRFMjGVSnfzMx6XmfB47m2hYhYCVwbEfe1pUl6VTfKmw58mtR38WhNeiupL6M03czMekFnl62m\nSPpIzfq+NesC3kyaQbCSPBbWiIi4RtJBpKfU2wwBHie1cErS25cxA5gBMG6cGyZmZo3SWfAYT7rr\nKfL6KtKlJUgtljFVC5E0ADiJ1OoAWMGmgWcsqS9jGLBrTfqYnL6iLV3SYGBb4K725UTEImARQEtL\nS7T/u5mZ9YzOgsfREXFTR3+UtHdBOacAX4+IZyXtROo03ysHla1yPZbm9EV5/6OB2yPiAUkXkVoU\n3wL2Bi6NiNaC8s3MrAd1GDw6Cxz578urFCDpDGAe8ElJAFuTWjVnAWcD64ETckf4KkmXSJoDDAWO\nz2XdKGmKpNmkZ05mVynbzMwao+pDgt0WEZ8lPd/R3t+Aq+psf2EH+zm3h6tmZmbdVHlUXTMzszYO\nHmZmVszBw8zMijl4mJlZMQcPMzMr5uBhZmbFHDzMzKyYg4eZmRVz8DAzs2IOHmZmVszBw8zMijl4\nmJlZMQcPMzMr5uBhZmbFHDzMzKyYg4eZmRVz8DAzs2IOHmZmVszBw8zMijl4mJlZMQcPMzMr5uBh\nZmbFHDzMzKzYoN6ugJn1XzNnzux23gULFvRgTazZ3PIwM7NiDh5mZlbMwcPMzIo5eJiZWTEHDzMz\nK+bgYWZmxRw8zMysmIOHmZkVc/AwM7NiTXnCXNIQ4D+BURHxoZw2ATgeWAdcHxHX5vQpwDRgK2Bx\nRNyS048EJgI7APMj4t5m1N3MzDbXrOFJhpGCwfYAkgRcAEyNiCcl/VzSklyf84DJwEDgauBgSWOB\nGRExVdLOwEJgepPqbmZm7TQleETEakl3A+Ny0r7A+oh4Mq//GTgS2ACsjIgNwAZJrZImA1OApXlf\nD0qaIGlsRNzXjPqbmdmmmtnnETXL+wBratZXAxMK0tcAezammmZm1pXe6jAfDjxas94KjOpG+iYk\nzZC0TNKytWvX9nilzcws6a3g8QiwTc36EODxbqRvIiIWRURLRLSMHDmyxyttZmZJbwWPFcCuNetj\ngeUF6WNyupmZ9YJmTgalmuXfAsMlDYmIZ4DdgR+T+kVOlTSAdHfWIFJH+f3AIgBJo4HbI+KBJtbd\nzMxqNOs5j12AA4C9JO0ZEXdKmgF8RtJDwJkR8VTe9izgbGA9cEJEbARWSbpE0hxgKOn5EDMz6yVN\nu1UXOLZd2s3AzXW2vQq4qk76hY2qn5mZlfHwJGZmVszBw8zMijl4mJlZMQcPMzMr5uBhZmbFHDzM\nzKyYg4eZmRVr5hPmZmYvmDlzZrfyLViwoIdrYt3h4PEi0N0voZlZd/mylZmZFXPwMDOzYg4eZmZW\nzMHDzMyKOXiYmVkxBw8zMyvm4GFmZsUcPMzMrJiDh5mZFXPwMDOzYg4eZmZWzMHDzMyKOXiYmVkx\nj6prZv3Klowi7eHce45bHmZmVszBw8zMijl4mJlZMQcPMzMr5uBhZmbFHDzMzKyYg4eZmRXzcx59\nxJbcu25m1XT3e+bnQzbnloeZmRVz8DAzs2L95rKVpCnANGArYHFE3NLLVTIze8nqF8FD0lDgPGAy\nMBC4Gji4VytlZvYS1i+CB3AYsDIiNgAbJLVKmhwRN/R2xczsxc+DMW6uvwSPfYA1NeurgQlAnwse\nvmvKzGq9WO/w6i/BYzhwb816KzCq/UaSZgAz8upTkv7UzfJGAA81Oa/LdJku02W+YOHChb3xOgF2\nq7JRfwkejwDb1KwPAR5vv1FELAIWbWlhkpZFREsz87pMl+kyXWZvl1miv9yquwLYtWZ9LLC8l+pi\nZvaS11+Cx+XAXpIGSNqa1GJa2st1MjN7yeoXl60i4mlJZwFnA+uBEyJiYwOL3JJLX93N6zJdpst0\nmb1dZmWKiEaXYWZmLzL95bKVmZn1IQ4e7UiaIulsSedJem1BviGS5kg6v7C8eZIekXSLpH8qzHuG\npD9LWi5pfEnenP+jkuZ2I98ySSFpnaSdCvOOkvQJSe+RtNnt1nW230rSQ7m8tn+Vb5yX9F5Jp0o6\nJd/KXTXfUZI+Jmm+pC5HM6j3+UuaIOnzkj4l6ZCCfIMknSTpu4XlfUDSmnxMHFSY91hJt0laKWmf\nqvlq/vYeSReVlJnTv1/zuf5jQb6hkj4u6WhJdW8t7eB1/qHdsXRuxXyHSDpT0omSPlryOiW9SdJ/\n5fPKUR3k2+w8UPVc1EHebp2PSvSLPo9m0ZYNgzKMNO7W9gXl7Q/cCowGPgtcAFS6vU7SROAq4Bxg\nMfBh4JSCsl8GHA98u2qenO8A4CzS3W6tEfFwQd5RwKXAOyNibcVsbwCOBG4GAvg88JOK5W0LnBQR\n++f130n674h4sot8Y4FjI2Kq0g0ayyXtGxFPdZJtk89fkkif59SIeFLSzyUtiYjnOsuXDaeDZ5k6\nKW88MBgYA3wIuFTSLh30DbbPuzOwKiJeLekzwJnAEV3layNpe2A28Ieq9c35dicdwycBGyNiTcV8\n2wI/BGZGxMqqZebgNA/4KbCR9J35dcXXeW5E/HPez2WSJkTEHyuUOYT03X5dREQ+Bn8dES88t1bv\nPJC/Z12eizo5hxSfj0q55bGpF4ZBiYjngVZJk6tkjIjVwN2F5d0fEf+TTyjzgM1+eXVS3h0RsSJS\np9VS4H8Ly/4AcElhHkgnpjcAQ0sCR7YI+HxB4AC4ISKui4gn88l754hYVTHvYGCipDGSBpKeD3q+\nQr7DgPsA8mdzN3BoZxnqfP77AutrAtWfSUGwq3zk97Wzk2JH+c6PiFbgi6QTx8gqeSPiwYj4ZV79\nLR0cS50c4ycB3ymtL+nkvTewSweBo6N8nwF+1EXgqJd3ZUR8LyKeyMfSP1MneHRQ5tiaX/+jqPOc\nWQd5DwCejL93Lv8OeE+7bPXOA1XPRXXPId08HxVx8NhUR8OgVFV090FE3FOzuh1wU0l+gPzLeA/g\nqwV5pgK/IP26LSlrIOnX5STg951dGqmT91XAm0hfwoslvbtKvvzFadvHPwK3VS0zIh4HLiT90pxD\n+qXa/pd/PUOAnWvWV1Ptqdvaz7/kWKp33FQ5ll7Ypt2xNBBY2678LvcvaQDwRtJJqFK+/JncR3qQ\ntyvty7wb2AX4raT2J9S6+XIrZwYwUNKFkj5Utcx2x9Jw0kl9Q8W6fgr4Sb5k9aWIuL9KmVQ4ljo4\nD1Q6fro4hzT0bigHj00NBx6tWe/q0kFPmg58uiRDbhKfA3wQOKNinsHA/hHxi9IK5l9B8yLiEOB0\n4OsF2Q8k/er6f8DHgG+osI8HeBvw48I8Z5A+x/cBnX3ha10HHCRpbA6YuwOlrazePJYOJbXwKp88\n8uucQ2qRfqGgrKNIl02LRcSXImI6qUX2lXzZuCv7A6tIrdhTgI9J6rRV2IG3ki4FVfVl0g+nU3L5\nVd0IvFxS2+XoV9D5sdR2HujO8VN8DtkSDh6bqjQMSk+TtCMwIiKuKckXEc9ExCzSyeLUitlmAF8r\nrGK9shcCT6h6h/kI4C8RsT7/WvoVqSVSooUUgEqcDxwC/By4NgfcTkXEzcCJpGvOHyRdWvllp5k2\n11vH0mBS66HoPv/8w+CTwOuA9+df5l2V9S7guyVBqoOyrwCuB/assPkI4L6IWBcRjwE/AqZ2o9ip\nlAWPs0mX2b4EXCNpTJVMuYXyduAMSaeQ+jDq/nBrdx4oOn66ew7ZEg4em2r6MCj5UsFJbMEvhoi4\nluqDoB0D3CjpHmAWMEvpAczuWMGmv4468zfSJYo29wPrqhaUvxwPF/6afjUwOvexnEhq+r+lSt6I\n+GZEvAe4B/h5RPylarlZbw2p8x/AZ7p7Qo+IO4A7SA/jduXdwI/ysfQ54EhJ3X047X/ZdPDTjmzR\ncQQvfOeG5suaVbYfDrw1Iu6KiE8D1wLvrFpeRFwZEUcCPyMFhes6qFPteaDy8dMT55DucPDY1JYO\ng6JulHkK8PWIeFbSTjXN284LkrbJd5203R10RZV8ETEpIsZHxHhgPjA/Ijq7xl1b5jBJE/LySODW\ngif9Lwf2zL+MIX0ZSi5BTSPdmVPiMfKvt1zPpcBfq2bOAWtm/lcpS83yb4HhNS2d3en49dY7bqoc\nS5tsI+nfgCsj4qF8q+aUKnklDcx9CUjaDvh9RDzdVb6IOKLmWDoN+F5EdHY7dG2ZW7d1QOf36IlO\nbqSofZ03AIPz8QfwcuD7Vcqs8XrS5aTO1OZ7tt36b+j8ONqszHw+mQcc3UFg3+Q8QAqKVc9FHZ1D\nunM+qsy36taILRgGRdIupDsr9pK0Z0TcWSHPGaQD6pPpzk62BsZXrO504GylZwEeJvUjNNorgSsk\n/Yz0Jf5S1YwR8Yikk0l1vg+4ICL+VlD2m4CTSyobEfdJukTSh0nv0Z8iYllX+fKJaT9SB+XRVe4q\nq/f5Kz1X8hlJDwFnRp1bfTvItwOphbSHpH3yZbRO85EurX0NeD4fS9sAdYNHnbw7A4slXQY8SLrt\nttJr7Op96aTMp0itluWkO54+V7VMpWcl5kq6FbgpIureaNJJfQ8l9b1Vqmsuc46kjwN/IX1P6was\nOq/zPtLnsBcwq96dgp2cB7o8F3WUd0s+q6o8PImZmRXzZSszMyvm4GFmZsUcPMzMrJiDh5mZFXPw\nMDOzYg4eZmZWzMHDmkJp/oWTlOYAOaImfXi+h/576mBehgr73lppjpDSca+6U9Y7Jc2WtELSfg0u\n6x2Sqjx1jaQ35Oc0zJrCz3lYU0n6FukhrddGxF9z2suBAyLiwi3Y71uBMyLiwB6paP0ydgR+GBFv\nlLQv8FxE3NLA8rYB1kVEl08K56eQx3RjGJXO9vkKYNeIWNJT+6zZ93ER8Y2e3q81j1se1mx/IQ0y\nuFhpJFdIE/Ns6a+YZ7cwfxUTSHUlIpY2MnDkMiq/poh4rocDx2DSMP89fo7Igwp+qqf3a83l4GG9\n4TjS2Fa1AzKOlnS1pLmStpN0gaSLlCZy+o7SdJyfl/RXSW+XdLrS9Ltza/YxQNJCSY9KOq8tUWkK\n2o9I+oWkPSW1SPpN3se9SrOxUbP9AflS2iclna80Fe4epEElxylN/7tDzfZvldQq6WBJkyU9mYdi\nQdJnJc3Ky7NzXb4p6fCcNktpdrl5klbktNPzdl+sKWM3pWlQPynpJ+3qu3XOf3len5H3ebzStKub\nnaiVhpu/UNJpkq7MLZdak0jjcb0rvx/LJH1L0ghJ10j6qaQdJU2UdL2k7SW9Xmnq3stqXv/2+ZLi\nf0m6XGnY9SnALrnsl3V8mFifFhH+539N+wfMzf+3AM+Qhg8fDxwLzK35+7HARXn5HOBi0iRH7yXN\nqzCcNCbTk3mbA4E/ATuRhvZ+mjS2zzTgtLzNTOAnefl3pFkRdweG1NRvO9IAdG2XdP8b+GhNGdd1\n8Lp+BByel78NHJ+XTyUNUPce4KyctiNpdNXxwETSAI4vy8tvAb6ct9slfUUD0nhT0/Ly++uUP62t\nbvn1P0SadGhX0qCD7bf/IGmsJfLrfX2dba4DDszLRwCX5eU3Ab/Oy/vk92UwaVBG8mfzdC77HKAl\np38XmJ2Xo7ePRf/bsn9ueViviDRA4cdIEwnt2MXm64C7Is369jfgkYh4PCIeJJ3s2zwQEQ9HGgTu\nalKAehPpV+6xpPmcH8jbPkMaFfjuiHimZh8HAU9FPsORRsI9vMJLWgy8W5JILfr3SxpEmoo2SBMe\n3Z1f+yPAMlKgeAZ4LCL+Gmk49PcCv8/bra7Z/xLSpb5zSSMU13uP2jybX8OqSPNJbDaPdUR8lTTI\n5b+TTvbtWx7tXQm8XtIw0gCZo3Nf1RtJc3H8A7Bjfp+PII3yPIL0/r82p6+icPZK67s8qq71moiY\nL+lA0miw/5fU79FTw0g/Tfr1/TLgjxFxEbzQsdwZsem0oQ9R7YT3Y9IQ94eT5ti+HDiBNPdDyX53\npM7JnhRs9gW+Atwg6TXR8RSqXcqXzQ6IiP+UdHRX20fEc5KuIAXBrUhzlh9DCuSRA2W0vc/ARfr7\nUOK/iog/5XK7ev+tn3DLw5qt/Q+WfwPa5mZ4Emiboe3VwLaF+xZA/vX/ClLr43rgzNwXsRtwfM32\n9Y7/JcAYpTnXyfu5tGb/db8zEbGONFf6eyPiNtLlrqMi4o95kx+TfpG32QVo67uo3edvgPfmfpa2\n+Vq2JU2jezdp+tRBwLCO3oSKTgBuVbqj6/+Q5sho/35vyOltLcPFpMB4BSl4nMbfZ8W7E9g992OM\nknRMfo3XAwsl7aE0f8e/5u03Sqrdt/UzDh7WNJL2Bg6X9C9taRHxKGlGulbSNfFpkn5E+mW+vaRJ\npLk1JilNevUW0pzQe7XtR9JhpNnv1kj6AvAJUj/HgxHxA9KJ/ArSHAzXKM2dvgfwDqXJj6ipz2Ok\nS0fnSzqN1BL4qtJsckcCEyW9uYOXuBi4pGa59rmLbwLLcyf1GcAX8mW3I0iXgN6at5tPmmjo16T5\nS+4D/oXUN/P9nPbN/L61va8DSdOqvjwHvUOBEZImSZqWt3lbu7r+kBQI5gF/BN7F5rMH/jRv84q8\nvoQ04dPdEbESuDgibs3v27Okfp0PALcDAyPNXTGXNHfHLcCZpL4hSBN7/Q+++tFv+TkPMzMr5paH\nmZkVc/AwM7NiDh5mZlbMwcPMzIo5eJiZWTEHDzMzK+bgYWZmxRw8zMysmIOHmZkV+//QO/aEMAqF\nYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aadf4d6ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "%matplotlib inline\n",
    "\n",
    "# Parse data. In this case there is no difference between 2-class and 3-class dataset\n",
    "filename = '../LSTM-CNN code/1_TrainingSet_2Class.csv'\n",
    "with open(filename, 'r', encoding='utf-8', newline='') as f:\n",
    "    def clean(sentence):\n",
    "        # cleaning process to remove any punctuation, parentheses, question marks. This leaves only alphanumeric characters.\n",
    "        remove_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "        return re.sub(remove_special_chars, \"\", sentence.lower())\n",
    "    reader = csv.reader(f)\n",
    "    training_tweets = []\n",
    "    for tweet in reader:\n",
    "        tweet[2] = clean(tweet[2])\n",
    "        training_tweets.append(tweet)\n",
    "\n",
    "# Find the distribution of number of words in tweets\n",
    "vectorizer = CountVectorizer(min_df=1, stop_words='english', ngram_range=(1, 1), analyzer=u'word')\n",
    "analyze = vectorizer.build_analyzer()\n",
    "\n",
    "# word2vec model from twitter based on 400 m tweets. Final results is for 3039345 m words with 400-dimension vector\n",
    "Word2Vec_model_T = KeyedVectors.load_word2vec_format('word2vec_twitter_model.bin', binary=True, encoding='latin-1')\n",
    "# word2vec model from pre-trained Google model with 1 m words and 300-dimension vector\n",
    "Word2Vec_model_G = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "numberofwords_T = []  # for twitter\n",
    "numberofwords_G = []  # for Google\n",
    "for tweet in training_tweets:\n",
    "    words_seq = analyze(tweet[2])\n",
    "    count = 0\n",
    "    for word in words_seq:\n",
    "        try:\n",
    "            a = Word2Vec_model_T[word]\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    numberofwords_T.append(count)\n",
    "\n",
    "for tweet in training_tweets:\n",
    "    words_seq = analyze(tweet[2])\n",
    "    count = 0\n",
    "    for word in words_seq:\n",
    "        try:\n",
    "            a = Word2Vec_model_G[word]\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    numberofwords_G.append(count)\n",
    "# Histogram of number of words per tweet\n",
    "labels = [['Google_word2vec', numberofwords_G], ['Twitter_word2vec', numberofwords_T]]\n",
    "for i, wor2vec in enumerate(labels):\n",
    "    plt.figure(i)\n",
    "    plt.rcParams['font.family'] = ['serif']  # default is sans-serif\n",
    "    plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    bin = np.linspace(np.min(wor2vec[1]), np.max(wor2vec[1]), np.max(wor2vec[1]) - np.min(wor2vec[1]) + 1)\n",
    "    plt.hist(wor2vec[1], bins=bin, histtype='bar', color='dimgray')\n",
    "    #plt.hist(numberofwords_G, bins=np.linspace(0, 22, 23), histtype='step', color='r', label='Google word2vec')\n",
    "    #plt.legend(prop={'size': 10})\n",
    "    plt.xticks(np.arange(0, max(max(numberofwords_G), max(numberofwords_T)) + 1, 1.0))\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Number of words in a tweet')\n",
    "    filename = 'Histogram_' + wor2vec[0] +'.png'\n",
    "    plt.savefig(filename, dpi=1200)\n",
    "\n",
    "\n",
    "# Summary statistics on number of words in each tweet.\n",
    "print('Mean of number of words for Google is {} and Twitter is {}: '.\n",
    "      format(np.mean(numberofwords_G), np.mean(numberofwords_T)))\n",
    "print('Median of number of words for Google is {} and Twitter is {}: '.\n",
    "      format(np.median(numberofwords_G), np.median(numberofwords_T)))\n",
    "print('90 quantile of number of words for Google is {} and Twitter is {}: '.\n",
    "      format(np.percentile(numberofwords_G, 90), np.percentile(numberofwords_T, 90)))\n",
    "print('Max of number of words for Google is {} and Twitter is {}: '.\n",
    "      format(np.max(numberofwords_G), np.max(numberofwords_T)))\n",
    "print('Min of number of words for Google is {} and Twitter is {}: '.\n",
    "      format(np.min(numberofwords_G), np.min(numberofwords_T)))\n",
    "\n",
    "# Find average number of missing words in word2vec models.\n",
    "num_miss_words = []\n",
    "for tweet in training_tweets:\n",
    "    words_seq = analyze(tweet[1])\n",
    "    count = 0\n",
    "    for word in words_seq:\n",
    "        try:\n",
    "            Word2Vec_model_G[word]  # or Word2Vec_model_T[word]\n",
    "        except KeyError:\n",
    "            count += 1\n",
    "    num_miss_words.append(count)\n",
    "ave_num_miss_words = sum(num_miss_words) / len(num_miss_words)\n",
    "print('Average number of missing words: ', ave_num_miss_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance monitoring plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEOCAYAAABrSnsUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYwzw//Qp8AOCxRZpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aadf4da198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = '../LSTM-CNN code/Accuracy-History-CNN-3class.pickle'\n",
    "with open(filename, 'rb') as f:\n",
    "    epoch, history_acc, history_val_acc = pickle.load(f)\n",
    "epoch = [i+1 for i in epoch]\n",
    "history_val_acc = [i * 100 for i in history_val_acc]\n",
    "history_acc = [i * 100 for i in history_acc]\n",
    "\n",
    "maxEpoch = np.argmax(history_val_acc)\n",
    "maxAcc_val = max(history_val_acc)\n",
    "maxAcc = max(history_acc)\n",
    "\n",
    "plt.figure(1)\n",
    "axes = plt.gca()\n",
    "x_min = epoch[0]\n",
    "x_max = epoch[-1]\n",
    "axes.set_xlim([x_min, x_max])\n",
    "axes.yaxis.grid(True, which='major')\n",
    "axes.yaxis.grid(True, which='all')\n",
    "axes.xaxis.grid(True, which='major')\n",
    "axes.set_ylim([round(min(history_acc), 0), 100])\n",
    "\n",
    "#plt.scatter(epoch, history_acc, color='r')\n",
    "plt.plot(epoch, history_acc, color='r', label='Accuracy on training set')\n",
    "#plt.scatter(epoch, history_val_acc, color='b')\n",
    "plt.plot(epoch, history_val_acc, color='b', label='Accuracy on test set')\n",
    "plt.xticks(np.arange(x_min, x_max, 2))\n",
    "plt.yticks(np.arange(93, 100, 1))\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.savefig('Train-Test Accuracy', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
